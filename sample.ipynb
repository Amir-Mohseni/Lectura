{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx-whisper\n",
      "  Using cached mlx_whisper-0.4.2-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting mlx>=0.11 (from mlx-whisper)\n",
      "  Downloading mlx-0.23.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numba in ./.venv/lib/python3.12/site-packages (from mlx-whisper) (0.61.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from mlx-whisper) (2.1.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from mlx-whisper) (2.6.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from mlx-whisper) (4.67.1)\n",
      "Collecting more-itertools (from mlx-whisper)\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting tiktoken (from mlx-whisper)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (from mlx-whisper) (0.29.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from mlx-whisper) (1.15.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface_hub->mlx-whisper) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->mlx-whisper) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->mlx-whisper) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->mlx-whisper) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface_hub->mlx-whisper) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub->mlx-whisper) (4.12.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba->mlx-whisper) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken->mlx-whisper) (2024.11.6)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->mlx-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->mlx-whisper) (3.1.5)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->mlx-whisper) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch->mlx-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->mlx-whisper) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->mlx-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->mlx-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->mlx-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface_hub->mlx-whisper) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->mlx-whisper) (3.0.2)\n",
      "Downloading mlx_whisper-0.4.2-py3-none-any.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading mlx-0.23.1-cp312-cp312-macosx_14_0_arm64.whl (27.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.6/27.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: more-itertools, mlx, tiktoken, mlx-whisper\n",
      "Successfully installed mlx-0.23.1 mlx-whisper-0.4.2 more-itertools-10.6.0 tiktoken-0.9.0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install mlx-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [02:48<00:00, 42.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import mlx_whisper\n",
    "\n",
    "result = mlx_whisper.transcribe(\n",
    "    \"samples/How Google's Transformer 2.0 Might Be The AI Breakthrough We Need.mp3\",\n",
    "    path_or_hf_repo='mlx-community/whisper-large-v3-turbo',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx_whisper\n",
    "\n",
    "speech_file = \"samples/How Google's Transformer 2.0 Might Be The AI Breakthrough We Need.mp3\"\n",
    "text = mlx_whisper.transcribe(speech_file)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So, large language models have a problem. It needs you to provide a context to understand and respond to your question properly. And to make it actually useful, you sometimes need the AI to reference an entire code base or documentation that can go up to millions of words in total. While this context window varies between the different models at the end of the day, it is still set to effects the mouse. So, extending that context window is the goal. Until we started to hit a wall of diminishing return. Not to include the fact that even if you can fit the information into the context window, the model can still forget or hallucinate. So, initially, researchers have looked for alternative mechanisms like mumbot. But the attention mechanism within transformers, which powers all the top AI models, is still rein the supreme in terms of performance. And most people pivot to storing memory externally like using rag. But that method relies too much on the selecting pipeline to find you to write documents to feed into the context window. So, the forefront of fixing the memory problem has shifted to improving the architectural designs surrounding attention instead. And today, we will be taking a look at three research papers that make these improvements at different scales. So, grab onto your copium inhalers because 2025 might just accelerate. And before we dive into it, aren't you tired of paying so many AI subscriptions every single month? What if I tell you there's a place the only newspaper 10 bucks and is actually pretty nice to use? MattMood.ai is an ON1 platform that provides you access to all the top tier text generation models, image generation models, and web search functionalities. With thumbnail offering 0.3 mini DPC-R1, clot 3.5, Gemini, Mejerni, Flux, and Recraft, all within a great price that is 10 bucks, it is already cheaper than paying for anyone of their monthly subscriptions. So, if you want to save some money while being able to experience the city of the R models, definitely check out MattMood.ai using the link down in the description. And thank you MattMood.ai for sponsoring this video. Anyways, in the first paper published by Sakana AI, the researchers thought, what if instead of just remembering everything in a systematic way, we have this new system called a NAM, which learns to evolve with the training data. To explain, as simply, think of a computer taking notes for a very long lecture on a piece of paper. The computer is systematic, so when the lecture is long, it uses predetermined algorithms to save writing spaces on the paper, like removing the space between words, or even skip writing the entire word, if it has more than, let's say, 7 letters. And when the lecture is way too long, the computer might end up writing down notes that just stored the initial letter of every single word that is spoken in the lecture. And practically, it is unreadable. So what's cool about NAM is that it is like an actual student that's been actively learning and taking notes from the lecture. So instead of blindly applying rules to cut down spaces, the student listen to the lecture and learn to identify what's truly important, like writing down the core ideas, key arguments, and the crucial examples. The student would also remove any redundant words, less important side comments, and unnecessary tangent that the professor made. And the student did not know how to take good notes at the very start. They were trained through their experience from not knowing how to make notes, rewarded when their notes helped them to get good grades, and evolved throughout their training stage to get to its insane note taking abilities now. So the student, which is NAM, is a 4,000 parameter model that is attached to the attention mechanism during training to learn what words are important and what's not, and can reduce 75% of the KV cash for LaMah 3AB, which just means that it can use 75% less space, compared to a computer writing down notes in a systematic way. And it can even be reattached directly to a different type of model, like when a smart undergrad student is tossed into a graduate level lecture, and can still have 17% less notes compared to a computer, and has no loss in performance, which is really cool, right? But that still doesn't solve the issue of only having one piece of paper to take notes on. And you cannot really just use two pieces of paper due to the guaranteed performance fall off of the attention mechanism. Basically, whoever reads more than one paper would have a stroke. So the next research by meta-code memory layer at scale is like giving the note taking student additional tool, which is a specialized flashcard system alongside their regular notes. For now, we had an improvement of the note take her, but here we are improving the paper, which is Transformers Dance layers that will provide space for the note take her to write down the core concepts, a reasoning and flow of the lecture. While this component is crucial for saving the lecture concepts, they are still computationally intensive to create and process. So memory layers are like this flashcard system that stores facts, where each flashcard has two sides, a key and a value. With a twist being that this flashcard system is trainable and self-organized during training. So as the student attends more lectures, the flashcard system learns to create and refine these key value pairs automatically, and because the key of the flashcard system is index, it is way faster than rereading all their detailed notes to find a single fact, which makes this process insanely cheap to run. But isn't this just using more papers? Well, it is, but the processing power needed for this well-organized system is so small that it barely impacts any performance. Then why don't we just make all the notes into flashcards then? Well, flashcards are only good for factual retrieval, right? A regular note is simply too versatile because it can contain more than just factual knowledge. Ideas such as how to reason effectively are stored within no rather than flashcards. So in the actual experiment of the research papers, the best ratio for replacing the dense layer with the memory layer is one in every eight layers. And it can increase the accuracy by two times on factual benchmarks and use this less compute as memory layers are sparsely activated. However, this does not take into consideration of the facts that might need constant updates, such as today's day, the current weather in New York, or it is now the Gulf of America. The ability to erase and forget about things is as important as the ability to remember things, right? So to implement that idea, the researchers over at Google proposed a more radical architectural change than the previous two papers. Inspired by the human brain and this architecture differentiates between memories just like ours. They design it with the ideas of a short-term memory along term memory and a persistent memory. The short-term memory is just like the note that the student is taking notes on. The long-term memory is kind of like the flashcards that the student uses, but not entirely the same, and the persistent memory is something a bit different, which I'll get to later. Apart from the short-term memory, that's mostly the same technical and analogy-wise that I use for the previous papers, the long-term memory on the other hand is a bit different. It is similar to flashcards, but not specifically designed with the efficiency that the key value technique has. It specifically has this surprise mechanism where it actively seeks out and prioritizes memorizing information that is unexpected or contradicts its prior understanding. Which creates this forgetting mechanism that is not just a simple decay, like a lazy student that doesn't check why he got a 99% on his math finals versus a hard-working student that revises why they got the question wrong. It is more open arms to new updates like the year now is 2050 instead of resisting. Anyways, as for the persistent memory, it is like the student learning skills and related to the lecture, and it is also not updated during inference time. Like you will learn how to write words outside of lectures and bring those skills into note-taking, and not trying to improve your handwriting during lectures, because you will be distracted too, right? This persistent memory is designed to store reasoning skills and other highly abstracted ideas that wouldn't need to be updated during inference time. So with these few new memories, the researchers have proposed three ways to combine them together when it is needed to generate an output. For memory as context, imagine the professor asks a question about a sub topic. So the student that is using Mac to answer would quickly pull up those flashcards and relevant background knowledge before answering. But the key here is that the flashcard is just for reference, and not necessarily used in every response. As for memory as a key, the flashcards are directly contributing to the response alongside its notes. So the flashcard is not like an afterthought. For memory as a layer, the students' response would be primarily looking at the flashcard to generate an answer. So these few ways are basically prioritizing the flashcards in different levels, with Mac being the best over all, Mac being the best for fast or parallel processing, and Mel being not as good. So with this new architecture, the context window now is able to scale larger than two million tokens like a breeze, which is highly effective in long context. With this Google's architecture code titans, I just realized I haven't mentioned its name. It can now perform any existing set of the art models in terms of accuracy in D1 million context window with a staggering 94% accuracy. What's even more insane is that it can extend up to a 10 million context window, which no other model has done before, and is still able to set out whopping 70% accuracy for basically finding a specific sentence within the entire Harry Potter series, Time's 10. Promising isn't it, as attention is still the key success for LLMs, this might be the evolution transformers needed. With the largest Titan model being only 760 million parameters, our eyes would need to focus on how well it scales next, so subscribe to stay tuned. And if you love these type of cutting-edge research, definitely check out my newsletter where I cover the latest and adjuice as papers weekly that I might not have the time to make into videos. I have already written a more technical explanation over there on titans when it just came out, and the same goes for the other two papers, so definitely go check them out. Thank you guys for watching a big shout out to Andrew Lascellias, Chris Ladou, Gigan, Bigulim, Robbers Aviasa, who is Muck, Ben Chainer, Marcelo Ferraria, Zion Sheep, and many others that support me through Patreon or YouTube, Follow me on Twitter if you have an and I'll see you in the next one.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
